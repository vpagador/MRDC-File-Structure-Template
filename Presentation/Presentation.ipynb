{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinational Retail Data Centralisation Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the aim and objectives of this project?\n",
    "\n",
    "## Aim\n",
    "#### To go through what its like to source, clean and centralise retail sales data to a database which can then be queried and for analysis\n",
    "\n",
    "### In this project you will be learning to:\n",
    "1. Extract data from various sources like AWS RDS, AWS S3 and PDF file \n",
    "2. Clean each extracted data \n",
    "3. Push the cleaned data as tables to a database\n",
    "4. Create the star-schema model in your database \n",
    "5. Query the database for sales analysis\n",
    "\n",
    "\n",
    "### Key Prerequisites\n",
    "1. OOP: Classes, methods and functions.\n",
    "2. Pandas: Reading from data sources, cleaning data and pushing data to a database \n",
    "3. AWS: boto3 for code and CLI for configuration \n",
    "4. SQL: For creating data model and querying the database\n",
    "\n",
    "### Other prerequisites\n",
    "5. APIs: For extracting the stores data\n",
    "6. tabula (and installation + configuration of Java): For extracting the credit card data\n",
    "7. Data and file types: YAML, JSON, CSV\n",
    "\n",
    "### Software needed\n",
    "- VSCode: IDE (Integrated Development Environment)\n",
    "- Conda: Package manager\n",
    "- pgAdmin4 or SQLTools: Interface for PostgreSQL \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestones Walkthrough\n",
    "\n",
    "## Milestone 1: Set up the environment\n",
    "- Create the repo on Github, then clone it into your VSCode to get started.\n",
    "\n",
    "## Milestone 2: Extract and clean data from the data sources\n",
    "- You're going to spend the most time on this milestone.\n",
    "\n",
    "#### Task 1: Set up a new database to store the data\n",
    "- This will be the destination of your data which in later tasks you'll extract from their original source and clean.\n",
    "- This is done in pgAdmin4 - You can't create databases in SQLTools\n",
    "\n",
    "#### Task 2: Initialise the three project Classes\n",
    "- Three files, one class per file.\n",
    "- Classes do not always have to have a class constructor method __init__\n",
    "\n",
    "#### Task 3-8: Extract and clean data \n",
    "- Each task focuses on a data source\n",
    "- Each task has subsequent steps which tell you what methods to create which will perform a particular job in your 'data centralisation pipeline'.\n",
    "    - A job will typically be one of: \n",
    "        a. Database utility (reading credentials, establishing a database connection, pushing data to your database)\n",
    "        b. Extracting from a data source\n",
    "        c. Cleaning data \n",
    "    - The jobs that your code will perform are categorised in this way and will be housed in one of your three classes you've created.\n",
    "- Each task will tell you where the data source is found. \n",
    "- The steps are in the order of ETL: Extract, Transform (or clean),  Load. \n",
    "    - There might be precusor steps which are to do with ensuring you have the necessary software and/or methods to create a connection with the data source iteslf, before performing the data extraction step. \n",
    "\n",
    "#### Milestone 2 TIPS:\n",
    "1. Before you try and establish a connection with AiCore's RDS Database or S3 Bucket, ensure that you configure your AWS CLI. Instructions to do this are in one of the lesson notebooks.\n",
    "2. Don't feel like you need to be super confident on all the prerequisites before you start this project. \"Take it as it comes\".\n",
    "3. Before doing any cleaning, have a way to look at your data in full once you've extracted it. Here are some ways:\n",
    "    - Exporting to a csv file\n",
    "        a. Opening the csv file in Microsoft Excel\n",
    "        b. Opening the csv file using the ExcelViewer Extension on VSCode\n",
    "    - Using pandasgui package \n",
    "4. You'll most likely write a lot of 'messy' code to test things out before implementing them in your methods. \n",
    "    - Use notebook files (.ipynb).\n",
    "    - Strip out functions/methods/class aspects of the code you want to test.\n",
    "5. If you're making changes to your code, make sure you make the change incremental and minimal, then test it after.  \n",
    "6. Create a main.py file. In this file you'll import the three scripts, instantiate the classes then run the methods, essentially running the whole data pipeline.\n",
    "\n",
    "## Milestone 3: Create the database schema\n",
    "\n",
    "#### Task 1-7: Set datatypes for each column of each table\n",
    "- Though you can do this manually in pgAdmin4, I'd use SQL to perform this.\n",
    "\n",
    "#### Task 8: Create the primary keys in the dimension tables\n",
    "- Use SQL for this.\n",
    "\n",
    "#### Task 9: Finalising the star-based schema & adding the foreign keys to the orders tables\n",
    "- Again, use SQL for this.\n",
    "- Sometimes the foreign keys fail to add because of missing data - most likely that might have been dropped during the cleaning phase\n",
    "- TIP: If you do face an error trying to create foreign keys, double check that you haven't prematurely drop rows from the data table, i.e., the dropped data might be legitimate\n",
    "\n",
    "## Milestone 4: Querying the data\n",
    "- This tests your SQL skills\n",
    "- Task 9 is the most complex query you'll build. The key is to build-and-test then build-and-test some more; start with a small query then build on it until it satisfies what the task specifies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
